\pdfoutput=1
\documentclass[conference,10pt]{IEEEtran} 
\IEEEoverridecommandlockouts

%\usepackage{draftwatermark}
%\SetWatermarkText{Draft}
\usepackage{balance}
\setcounter{tocdepth}{3}
%\usepackage{cite}
\usepackage{cite}
\usepackage{graphicx}
%\usepackage{showframe}
\usepackage{enumitem}
\usepackage[skins]{tcolorbox}
\usepackage{dblfloatfix}  
\usepackage{colortbl}
\usepackage{arydshln}
\usepackage{times}
%\usepackage[dvipsnames]{xcolor}
\usepackage{rotating}
\usepackage{makecell}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{tikz}
\usetikzlibrary{angles}
\usepackage{makecell}
\usepackage{tabu}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{framed} 
\usepackage{newtxtext,newtxmath,amsmath}
\usepackage[framemethod=tikz]{mdframed}
\usetikzlibrary{shadows}
\usepackage{graphics}
\newmdenv[
tikzsetting= {fill=gray!10},
linewidth=1pt,
roundcorner=2pt,
shadow=false
]{myshadowbox}
%\usepackage[framed]{ntheorem}

\newcommand{\bluecheck}{}%
\DeclareRobustCommand{\greencheck}{%
  \tikz\fill[scale=0.25, color=green]
  (0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;%
}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newenvironment{result}[2]
{\begin{myshadowbox}\textbf{\textit{\underline{Lesson#1:}}} #2}{
\end{myshadowbox}}

\makeatletter
\let\th@plain\relax
\makeatother

\hypersetup{
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\newcommand{\tikzhighlightanchor}[1]{\ensuremath{\vcenter{\hbox{\tikz[remember picture, overlay]{\coordinate (#1 highlight \arabic{highlight});}}}}}

\setlist[itemize]{leftmargin=0.4cm}
\setlist[enumerate]{leftmargin=0.4cm}

\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}
\newcommand{\tion}[1]{\S\ref{sect:#1}}

\newenvironment{RQ}{\vspace{1mm}\begin{tcolorbox}[enhanced,width=3.4in,size=fbox,colback=red!5!white,drop shadow southeast,sharp corners]}{\end{tcolorbox}}

\usepackage{url}
%\newcommand{\keywords}[1]{\par\addvspace\baselineskip \noindent\keywordname\enspace\ignorespaces#1}
%%% graph
\newcommand{\crule}[3][darkgray]{\textcolor{#1}{\rule{#2}{#3}}}

\tikzstyle{thmbox} = [rectangle, rounded corners, draw=black, fill=gray!10]
\newcommand\thmbox[1]{%
	\noindent\begin{tikzpicture}%
	\node [thmbox] (box){%
		\begin{minipage}{.94\textwidth}%
		\vspace{-0.1cm}#1\vspace{-0.1cm}%
		\end{minipage}%
	};%
	\end{tikzpicture}}

\newcommand{\quartex}[4]{
\begin{picture}(25,6)%1
    {
        \color{black}
        \put(#3,3)
        {\circle*{4}}
        \put(#1,3)
        {\line(1,0){#2}}
    }
\end{picture}
}


\begin{document}
\title{The Changing Nature of Computational Science Software}

\author{\IEEEauthorblockN{Blinded for review}}


% \author{\IEEEauthorblockN{Huy Tu}
% \IEEEauthorblockA{NC State University\\Raleigh, USA\\hqtu@ncsu.edu}
% \and
% \IEEEauthorblockN{Rishabh Agrawal}
% \IEEEauthorblockA{NC State University\\Raleigh, USA\\ragrawa3@ncsu.edu}
% \and
% \IEEEauthorblockN{Tim Menzies}
% \IEEEauthorblockA{NC State University\\Raleigh, USA\\timm@ieee.org}
% }



% The paper headers
\markboth{IEEE Conference on Software Engineering}%
{Tu \MakeLowercase{\textit{et al.}}: Changing Nature of CSc Software for IEEE Journals}

% make the title area
\IEEEtitleabstractindextext{
\begin{abstract}

How should software engineering be adapted for Computational Science (CSc)? If we understood that, then we could better support software sustainability, verifiability, reproducibility, comprehension, and usability for CSc community. For example, improving the maintainability of the CSc code could lead to: (a) faster adaptation of scientific project simulations to new and efficient hardware (multi-core and heterogeneous systems); (b) better support  for larger teams to co-ordinate (through integration with interdisciplinary teams); and (c) an extended capability to  model complex phenomena. 

In order to better understand computational science, this paper uses quantitative evidence (from 59 CSc projects in Github) to check 13 published beliefs about CSc. These beliefs reflect on (a) the implications of limitations of computer hardware; (b) the nature of scientific challenges;  and (c) the cultural environment of scientific software development. What we found was, using this new data from Github, only a minority of those older beliefs can be endorsed. More than half of the pre-existing beliefs are dubious, which leads us to conclude that the nature of CSc software development is changing. 

Further, going forward, this has implications for (1) what kinds of tools we would propose to better support computational science and (2) research directions for both communities.

\end{abstract}

\begin{IEEEkeywords}
Beliefs, Mining Software Repositories, Computational Science, Empirical Software Engineering
\end{IEEEkeywords}}
\maketitle
\IEEEpeerreviewmaketitle
\IEEEdisplaynontitleabstractindextext


%
%
\section{Introduction}
  

Computational Science (hereafter, CSc)
field studies and develops software   to explore
 astronomy, astrophysics, chemistry, economics, genomics, molecular biology, oceanography, physics, political science,  and many   engineering fields 
There is an increasing reliance of computational methods software for science. For instance, a Nobel Prize in 2013 went to chemists using computer models to explore chemical reactions during photosynthesis. In the press release of the award, the Nobel Prize committee wrote:

\begin{quote}
{\em Today the computer is just as important a tool for chemists as the test tube \cite{nobel_2013}.}
\end{quote}


Computational scientists explore software models than manually explore the physical effects they represent because it is done in real-time, more precise, faster, cheaper, and safer.  For instance, in material science, CSc explores the properties
of new materials by synthesizing them, which is very expensive so standard practice is to use software to determine
those properties (e.g. via a finite element analysis). This, in turn, enables (e.g.) the faster transition of new materials to industry. Moreover, scientific software have important and widespread impacts on our society. Specifically, in weather forecasting, predictions generated from CSc
software can tell the estimated path of hurricanes. This, in turn,
allows (e.g.) affected homeowners to better protect themselves from
damaging winds. 

%From these examples, there are multiple good reasons for increasing reliance on computational methods software for science such as it is real-time, more precise, faster, cheaper, and safer to explore software models than manually explore the physical effects they represent. For instance, CSc software can explore the effects of several 
%hurricanes 
%scenarios and nuclear reactions without risk to human life
%or property~\cite{heaton15_lit}.

There is much demand for better software engineering (SE) methods
for CSc. For example, an investigation of the 
quality of scientific software during the ``Climategate'' scandal \cite{merali10_error} found little to no results reproducibility. Recent high-profile retractions, technical comments, and corrections
because of software errors include papers in  \textit{Science} \cite{Chang1875_science, comments_science}, the \textit{Journal of Molecular Biology} \cite{Geoffrey_JMB}, \textit{Ecology Letters} \cite{lee_ecolet, david_ecolet}, etc. Improving the verifiability and maintenance of CSc software would hence increase and ensure the credibility of CSc results and implications. Table \ref{tab:characteristics}
lists some of the prior results
where empirical software
engineering researchers have explored computational science
(this table comes from the work of
Carver, Heaton, Basili, and Johanson \cite{carver13_perception, carver07_environment, basili08_hpc, heaton15_lit, johan18_secs}, and others).
Johanson et al. \cite{johan18_secs}   argues that SE practices will only be integrated into CSc when
those practices take advantage of
the   13 beliefs  of
Table~\ref{tab:characteristics}. 
 

\input{difficulties.tex}

Just  because  prior research endorsed
 ``X'' does not mean that  ``X'' is relevant in the current context. There are numerous examples of long-held beliefs which, on re-evaluated, proved to be incomplete or outdated~\cite{menzies17,dev16}. 
Given that, and the prominence  of 
computational science, it is 
well past time for a second look at the beliefs of 
Table~\ref{tab:characteristics}. 

A recent trend is that CSc researchers store their code on open source repositories (such as Github).
Our study of the 13 beliefs mines the code and comments of dozens of the repositories of  those CSc projects. Four of those beliefs cannot be explored using Github data. For the remaining:
\bi
\item Assuming each belief held,
we described what effect   we would expect to see in project data,
\item Then, we check if that effect actually exists in the data.
If so, then  we  {\em endorse} that belief. Otherwise, we have cause(s) to {\em doubt} it. 
\ei

Based on the analysis of 59 CSc projects as proxies for CSc software, our findings and contributions include: 
\be
\item Contrary to prior research, only small number of proposed beliefs in \cite{johan18_secs} are endorsed. As discussed at the end of this paper, this has implications for research practices and tools we would propose to better support CSc. 
\item The relevance of the scientific software development beliefs may change according to time.
In this regard, it is appropriate to note that
  much of the prior analysis that leads to Table \ref{tab:characteristics} was qualitative in nature (i.e. impossible to reproduce, check, or refute). This work, on the other hand, is quantitative in nature. Hence, it can be be reproduced/improved or even refuted when.  To assist in that process,  we have posted all our data and scripts at
\url{https://github.com/se4cs/se4cs}. 
\ee

The rest of this paper is structured as follows.
The next section offers some preliminary notes on the data
we collected with our methods for labelling and analyzing
that data. Then \S3 discusses the general threats to validity of our work. \S4-6 provide background, analysis results for much evidence
of the changing nature of computational science software. \S7-8 conclude and offer future directions of SE for CSc research. 

% Orior researchers 
% There have been definite efforts in employing these modern SE practices - which was proven to improve the traditional software - for the CS field. This hopefully would be of great assistant to the computational scientists in the fields such as molecular dynamics, quantum chemistry, and computational materials. However, there have not been large scale empirical study and results indicating the validity of these improvement observations while recommending actionable changes for these observations to hold in the scientific context.       

% It is important first to identify and understand how scientific software development differentiates from Software Engineering (hereafter, SE) development. Faulk et al. and Hannay et al. \cite{hannay09_secs, faulk09_secs} note that a ``wide chasm'' of how these two fields are speaking a common language, yet ``separated'' by upholding to a different cultures and values:
% \bi
% \item
% Software development practice has aimed for generality of ``all things applied'' perplexing computational scientists focusing on ``domain specific''. 


% It is inevitable that the two fields must not continue existing in isolation. However, the gap bridging progress of modern SE practices in computational science has remained status quo through 13 recurring underlying causes results from the nature of scientific challenges, from limitations of computers, and from the cultural environment of scientific software development that developed and restated through this past decade by Carver, Heaton, Basili, and Johanson \cite{carver13_perception, carver07_environment, basili08_hpc, heaton15_lit, johan18_secs}. Johanson et al. \cite{johan18_secs} specifically claims that SE practices will only be integrated if they honored the mentioned 13 characteristics and constraints of scientific software development mentioned in the Table \ref{tab:characteristics}.
% Traditional SE environments such as businesses or IT companies have used SE practices, it is puzzling of how the scientific software developers not using them or not using them effectively. Throughout literatures, there have not been a quantitative study of these 13 characteristics and constraints in order to give a more empirical view of the scientific software development practices in comparison with the traditional SE practices. 

 
 

\section{Preliminaries} 

%The growing dependency of science on computational methods software to make decisions for scientists is inevitable. By enhancing the software's quality, scientists can guarantee the CS work more credible and more productive. Therefore, 
%better SE improves computational
%science software, which would lead to better (e.g.) weather prediction and the faster creation of new industries based on new materials.

\subsection{Modelling Assumptions (and ``Indicators'')}\label{model}

The reasoning of this 
paper makes
modeling assumptions in order to bridge between the higher-level concept of the belief to what are measurable through the Github data. For example, consider the belief 2.b ``Verification and validation in software development for CSc are difficult''.  
Having read 10,000s of comments, we can assert that 
very few commits are labelled  ``verification and validation'' (V\&V) and, of those that are,
even less use these terms in a consistent manner. Instead, based on our reading of the commits, we could assign labels showing whether or not developers were reporting the results of creating/running tests. Hence, to explore that belief we had to make the following modeling assumptions to bridge between the terminology of the belief and the terms in the Github data: (1) V\&V is associated with testing; and (2) the amount of testing is an indicator for V\&V activity. This modelling assumption that relies on commits to indicate the amount of developers effort in a specific  task is also done by other SE researchers \cite{vasilescu16_limit, xia2019sequential}. In order to materialize and fathom the belief's context, it is imperative to have an establish standard  to benchmark results from CSc against, in which SE would be the ideal candidate. For instance, to understand the difficulty of V\&V in CSc projects, we measured the indicators based on (1)-(2) in both CSc and SE projects and then compared them. 


Formally, this means that our conclusions are based on what
Schouten et al. describe as
 {\em indicators}~\cite{schouten2010indicators} rather than direct measures. Indicator-based reasoning is often used as a method to take steps closer to intangible/ abstract/ expensive vision. For example,
 in statistics, Schouten relied on indicators to support large survey data collection monitoring \cite{schouten2010indicators}. Also, in SE, Lamsweerde used indicators to evaluate the degree of fulfillment of goals \cite{vanLamsweerde2009_requirement}.
 Further, in business 
 management, Kaplan and Norton \cite{kaplan1996using} offered a four-layer ``perspectives diagram'' that implements the bridge from high-level and intangible business goals down
 to observable entities, i.e. indicators (at the time of this
 writing, that paper has  9800+ citations in Google Scholar).
   

% \subsection{Research Questions}

 
% There have been many claims and many studies made about how scientists develop software along with systematic reviews of the literature from both scientists and SE community about developing software. It is essential to not only survey literatures of both community but also conducting quantitative study and interviews to validate the 13 characteristics:

% \begin{enumerate}
%     \item What claims have scientists indicated within the characteristics/constraints about why state of the art SE techniques are poorly adopted by scientists?
%     \item What empirical evidence (e.g. quantitative study of the project development or interviews) to validate or reject these claims? 
% \end{enumerate}
 
 
\subsection{Data Collection}\label{tion:data}

\begin{wraptable}{r}{1.5in}
\vspace{-15pt}
\centering
\caption{Data sanity checks. From \cite{Kalliamvakou:2014}.}\label{tbl:sanity}
\small
 
%{\small
 \begin{tabular}{r|l}
 Check   & Condition    \\\hline
 \# Developers & $\geq$ 7 \\
 Pull requests  & $>$ 0 \\
Issues & $>$ 10 \\
Releases &  $>$ 1 \\
Commits & $>$ 20 \\
Duration  & $>$ 1 year 
\end{tabular}%}
\vspace{-10pt}
\end{wraptable}
To check our beliefs on CSc projects, we proceeded as follows. 
Using our contacts in the CSc community, from the Molecular Sciences Software Institute (MOLSSI), and the Science Gateways Community Institute (SGCI), we found
678 CSc  projects.
Researchers
warn against using all the Github data~\cite{bird09promise,agrawal2018we, eirini15promise, munaiah17curating} since
many of these projects are simple one-person prototypes.
Following  their advice, we applied the sanity checks of Table \ref{tbl:sanity}
to select 59  projects with sufficient software development information
(for space reasons, we list those projects outside of this paper in our on-line materials; see \url{https://github.com/se4cs/se4cs}).
 
\definecolor{amethyst}{rgb}{0.6, 0.4, 0.8}\definecolor{cadetblue}{rgb}{0.37, 0.62, 0.63}
\begin{figure*}[!t]
\vspace{-5pt}
\centering \includegraphics[width=.95\linewidth]{img/summary.png}

\caption{Data distributions from 1300 SE projects (shown in \colorbox{cadetblue}{\textcolor{white}{teal}}) \& 59 CSc projects (shown in \colorbox{amethyst}{\textcolor{white}{purple}}).}\label{fig:comparison}
\vspace{-13pt}
\end{figure*}    



Figure~\ref{fig:comparison} shows some statistics on the data we collected from our 59 CSc projects. For comparison purposes, we compare that sample to 
a sample of 1037 Github projects from~\cite{Majumder19}.
There is no overlap between the CSc projects and the Github sample. Also, all the Github
sample passes the sanity checks of Table \ref{tbl:sanity}. Figure~\ref{fig:comparison} uses the following terminology.


\textit{Developers}: are the contributors to a project, who code and submit their code using commit to the code base. The number of developers signifies the interest of developers in actively participating in the project and volume of the work.
  
  

\textit{Commits:} in version control systems, a commit adds the latest changes to the source code to the repository, making these changes part of the head revision of the repository. 

\textit{Open \& Closed Issues:} Users and developers of a repository on Github use issues as a place to track ideas, enhancements, tasks, or bugs. As they work, they open issues with Github. When developers address those matters, they close the issues.

\textit{Tags}: are references that point to a specific time in the Git version control history. Tagging is generally used for marking version release (i.e. v1.0.1).


\textit{Releases:} mark a specific point in the repositoryâ€™s history. The number of releases defines different versions published (and  signifies a considerable  changes  between each version).

\textit{Duration:} The duration of a project marks the length of the project from its inception to the current date or project archive date (in week as a unit of time). It signifies how long a project has been running and in the active development phase.





\textit{Stars:} signify how many people
  ``liked'' a project's repository enough to bookmark to get updated on its future progress.
  
   \textit{Forks}: A fork is a copy of a repository. Forking a repository allows users to freely experiment with changes without affecting the original project. This number
  is an indicator of how many people are interested in the repository and actively thinking
  of modification of the original version.
  
   \textit{Watchers}: are GitHub users who have asked to be notified of activity in a repository, but have not become collaborators. This is a representative of people actively monitoring projects, because of possible interest or dependency.
   
  
  The following observation  
  will become important, later in the paper (i.e. issues of size conflation).
Assuming that ``standard'' SE projects are those we see in Github which pass the sanity checks of Table \ref{tbl:sanity}, then
 Figure~\ref{fig:comparison} shows that
  \begin{quote}
  {\em It is not true that CSc projects are usually smaller than standard SE projects.}
  \end{quote}
 To justify this statement, we applied 
  a 95\% confidence bootstrap statistical test~\cite{efron94} and an A12 effect size test~\cite{arcuri2011practical}, to all the  Figure~\ref{fig:comparison} distributions where the median CSc values were lower than the median SE values.
Only in the case of {\em duration} were the median CSc values statistically different and less than the SE medians. All the other indicators show that CSc projects are just as active (or even more active) that SE projects 

The one clear negative result for CSc projects of Figure~\ref{fig:comparison} is that their {\em duration} is less than the duration of the SE projects (281 weeks versus 409 weeks). 
It suggests
that the CSc developers are working
just as hard (or even harder)
as the SE community, {\em and does so in less time}.
This suggests that SE has more to learn from CSc than the other way around.
If we say that   an {\em efficient} software process is one that allows  people to work together, faster,  then 
Figure~\ref{fig:comparison} is saying:
\begin{quote}
{\em CSc software development is  more efficient that SE.}
\end{quote} 

     
 \subsection{Labelling}\label{tion:labelling}
 When code is shared
within a software repository, an important event is the {\em commit comments}. These comments are the remarks developers make to document and justify some updates to the code. Code repository systems such as Github store tens of millions of these comments that serve as a rich source of information about a project within SE literature. For instance, within SE literature,  Vasilescu et. al \cite{vasilescu16_limit} and Menzies et. al \cite{xia2019sequential} studied commits as an indicator for development effort of projects.


To understand the scientific development process, we manually categorized the commit comments seen within CSc
software. 
Using the power of free pizza, we assembled a team of 10 computer science 
graduate students.
To allow other researchers to reproduce this work, we set
the following
resource limit on our analysis.
According to Tu et al.~\cite{tu2019better}, two humans can manually read and categorize
and cross-check 400 commit comments per day (on average).
Hence, for this study, for each project, we categorized 400 commits
(selected at random). 
All in all, our
reviewer team spent 320 hours (in total) categorizing comments. Our  reviewers
labeled commits using the following
guidelines:
\bi
\item {\em Science enhancement}: any core science (e.g. an equation of Pascal triangle) that is being implemented or modified.
\item {\em Engineering enhancement:} any other enhancements that related to code complexity (e.g. data structures \& types, I/O formats, etc) 
\item {\em Bug fixes:} Fixing software faults reported or found within the development. 
\item {\em Testing: } evaluate the functionality of a software application (e.g. scientific calculations to output/input formats).
\item
{\em Other:} not core changes, e.g. renaming or formatting 
\ei

 

Each commit was labeled by two reviewers,
neither of which had access to the other's labels. Moreover, the reviewers did not only look at the commit message but also the code contribution associated with the commit (e.g. to determine if the nature of some enhancement was 
``scientific'' or ``engineering'' in nature). The inter-rater reliability between the two reviewers where the Cohen's $\kappa = 0.74$, which is a ``substantial agreement'' \cite{irr_kappa}. When labels disagreed, the commit was given to our most experienced reviewer who made an executive decision about what
was the correct label. 

\subsection{Beliefs We Cannot Explore (Using Github)}

Github stores data about code and the comments seen during code reviews and pull requests. While this is useful for assessing most of the beliefs of Table~\ref{tab:characteristics}, it does mean that four of the thirteen beliefs, summarized by Johanson et al. \cite{johan18_secs}, cannot be explored by this paper:

\be
\item {\em Project Requirements are not Known Upfront}: The project requirements documents from these scientific teams are not readily accessible through Github which is prerequisite to map it to the developments.   
\item {\em Overly Formal Software Processes Restrict Research}: Computational scientists perform many tasks,
only one of which is developing software. For example,
they must write grants, do presentations, traveling, keeping up with the fast-developing fields, etc. Hence, measuring the formality of software processes and research efforts would be outside of the scope for Github.
\item {\em Development is Driven and Limited by Hardware:}
From our Github data, the information about hardware platforms is found to be difficult to access. Hence, we cannot reason about this belief.
\item {\em Conflicting Software Quality Requirements:} These requirements include functional correctness vs
performance or portability or maintainability. Specifically, performance issues conflict with portability and maintainability since these are often achieved via hardware-specific optimizations. As with issues
relating to hardware,
the information rarely exists on Github. 
\ee

\section{Threats to Validity}

\subsection{External Validity}
Like any data mining paper,
the results of the following analysis are skewed by sampling bias.
To combat that effect, when we analyze Github data, we took care to analyze as much as possible.
Hence,
as a starting point of this work, we looked at 678 CSc projects. 
Using the advice from Kalliamvakou et al.~\cite{Kalliamvakou:2014}, we applied certain sanity checks of Table~\ref{tbl:sanity}  to focus on 59 of those 678 projects.

At 59 projects, this sample is much larger than seen in
most prior studies on computational
science. That said, it is certainly true that another sample of different projects would make different conclusions.
Accordingly, we make all our scripts and data publicly so that
(a)~our current conclusions
are repeatable/ refutable/ improvable can be quickly repeated across multiple projects by anyone with access to Github; and (b)~our current conclusions can be checked against other data, whenever that becomes available.


\begin{figure*}[!t]
\vspace{-15pt}
\begin{center}
\resizebox{0.8\textwidth}{!}{
 \includegraphics[width=.17\textwidth]{img/CS_commits_1.png}
\hspace{2mm}\includegraphics[width=.19\textwidth]{img/SE_commits_1.png}}
 
\end{center}
\vspace{-10pt}
\caption{Distribution of development within our sample of 59 CSc projects (left) and 20 top sampled SE projects (right).}
\label{fig:SE_activities}
\vspace{-5pt}
\end{figure*}

\subsection{Construct Validity}
As mentioned above, the following analysis depends on numerous {\em indicators} to bridge between the belief being explored and the available data. 
The {\em modeling assumptions} are documented carefully  used to design those indicators.

\section{Limitations of Computer Hardware Beliefs}

% In this section, we discuss characteristics of software development in
% computational science that are due to limitations regarding available computing
% resources and their efficient programming. 

\subsection{Use of ``Old'' Techniques (and a Disregard for  most Modern SE Methods)}\label{lang}
This section explores belief 1b (CSc teams use ``old'' SE techniques)
and, as a side effect, belief 3b (CSc disregards most modern SE methods).

\noindent \textit{\underline{Belief:}} According to Basili et al., and others~\cite{basili08_hpc, carver07_environment, Prabhu11_cssurvey, kendall05_C, ragan14_pythoncs},
computational scientists prefer
``older''-style programming languages and technologies while disregarding most of the newer SE methods

\noindent \textit{\underline{Notes:}} The usual argument here is that CSc Scientists are skeptical of modern SE methods and new technologies/languages.
This is based on several factors: 
\begin{itemize}
  \item A decades-long commitment with these older-style languages (Fortran and C) on high-performance computing platforms \cite{faulk09_secs}.
  \item A belief that the extra features of the newer languages needlessly conflate functionality that can be more easily implemented in (e.g.) one line of ``C'' macros \cite{sanders08_risk}. 
  \item A prejudice against the never languages or a perception that the scientists would not find then useful \cite{Prabhu11_cssurvey}. 
\end{itemize}



\noindent \textit{\underline{Modeling Assumptions:}} 
One indicator of using ``new'' techniques is the presence of automatic testing and deployment tools; e.g. use of the Travis CI tool that re-runs test suites whenever new code is committed to a repository. 

Another indicator is the development language for the project. 
Johanson et al.~\cite{johan18_secs} say that, in CSc, Fortran and C are examples of this ``old'' technology. The use of C++ is an interesting borderline case- Johanson et al. regard that as ``new technology'' even though it is now decades old. In the following, we will consider the C++ data as a special case.




\noindent \textit{\underline{Prediction: }} If CSc teams are mostly focused on ``old'' technology then most of those projects would use ``old'' languages and would not use automated testing
tools like  Travis CI.


\begin{wraptable}{r}{1.5in}
\vspace{-10pt}
\caption{Languages in 59 CSc projects.  
}\label{tbl:language}
\vspace{-12pt}
 \footnotesize
%\begin{threeparttable}
%\vspace{-10pt}
%\resizebox{!}{0.2\linewidth}{
%\setlength        abcolsep{10pt}
 \hspace{-3pt}\begin{tabular}{l|c|c}
 \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{Count} & \multicolumn{1}{c}{Percent}\\
\hline
Other & 3 &  5\%  \\ 
Javascript	& 2 & 3\% \\ 
C &	3 & 5\% \\ 
Java	& 5 & 9\% \\ 
Fortran	& 6 & 10\% \\
C++	& 17 & 29\% \\
Python & 23 & 39\% 
\end{tabular}
\vspace{-10pt}
%}
%\end{threeparttable}
\end{wraptable} \noindent\textit{\underline{Observed:}} 
As seen in Table~\ref{tbl:language}, C and Fortran are just 15\% of our sample.
Even if we call C++ ``old'', then the ``older'' technologies of Table~\ref{tbl:language}
cover less than half the sample (44\%).
As to other measures of ``new'', we found that  $43/59=73\%$
have active
Travis CI connections.  Hence, we say:

\begin{RQ} 
\textit{\underline{Conclusion:}} We \textbf{doubt} the belief that 
CSc developers are skeptical of modern SE methods and new technologies/languages.
\end{RQ}

\noindent \textit{\underline{Discussion:}} This result is at odds
with numerous papers~\cite{basili08_hpc, carver07_environment, Prabhu11_cssurvey, kendall05_C, ragan14_pythoncs}. 
Most of the papers that endorse this view come from before the recent Silicon Valley boom. In our recent discussions with postdocs and Ph.D. students working on CSc projects,
they were well aware of the potential salaries if they are adept the popular tools used by contemporary agile software companies. Moreover, there is and advent of Research Software Engineers, an SE-adjacent role in research computing \cite{RSEs_roles}, as well as the availability of low-threshold educational activities in
research computing such as The Carpentries. Thus, it is perhaps not so surprising that we report here a widespread use of modern software techniques in CSc.
 


\subsection{Cannot Separate Domain Logic \& Implementation Details} 






\noindent \textit{\underline{Belief:}} 
According to Vanter et al.~\cite{faulk09_secs},
CSc developers working on scientific software development do not separate high-level domain logic with
lower-level implementation details. If true, this would somewhat restrict the ability
of this community to develop general-purpose abstractions. This, in turn, would lead
to productivity issues since new applications would have to rework much of the previous
work. 



\begin{figure}[!t]
\begin{center}\includegraphics[width=0.95\linewidth]{img/commits_belief1.png}\end{center} 
\vspace{-15pt}
\caption{Median percent  of total commits seen
at 10, 20, 30, ... 100\% of 
the time these CSc projects were
documented in Github.
X-axis measures time as percent of days seen in Github.
Note
that all commit types occur
at a similar, and near constant,
 rate, across the lifetime of a project.}\label{fig:belief1} 
\vspace{-5mm} 
\end{figure} 



\noindent \textit{\underline{Notes:}} One measure of the mature software engineering is the
use of abstraction; i.e. the ability to step back from application-specific
details to generate domain-general abstractions. Such abstract thinking is harder to do for developers that spend more time studying physical phenomena than the code used to model that phenomena.


\noindent \textit{\underline{Modeling Assumptions:}} During CSc software development,
\bi
\item Domain logic addresses computational models, i.e. core science, understanding (as manifested by scientific enhancement activities). 
\item Implementation details address coding/building   tools   to solve scientific problems (as manifested by engineering enhancement). 
\ei


The level of cohesion between these two activities type can be shown in (1) frequencies and (2) the amount of shared artifacts. For shared artifacts, we relied on files that were maintained by both activities and the commits that add those files to the source code. 

\noindent \textit{\underline{Prediction: }} If domain logic and implementation details are intermingled during the development of scientific software, then both scientific and engineering enhancement contribution should occur at similar frequencies and the artifacts are shared considerably high. 



\noindent \textit{\underline{Observed:}} Specifically, this belief analysis can only be completed within CSc projects as there is no scientific activities specifically for SE projects. Across the enhancement type commits from the sample (from Figure \ref{fig:SE_activities}), 25\% enhancement commits focusing on the core science while the rest 27\% enhancement commits focusing on the quality of the code. The absolute difference between the two types of enhancement activities is small (2\%). Moreover, from Figure \ref{fig:belief1}, the rate of commits of engineering (red) and scientific (yellow) enhancement activities are observed to grow synchronously across the project lifetime. 

If both activities occur synchronously while working on the same artifacts, then CSc developers cannot perform one type of activity without performing the other. In regards to shared artifacts, the amount of relevant overlapped files (not documentation or binary ones) and the commits that maintaining those files are documented. The majority of projects would need to below or above certain $M$ threshold to make a conclusion. Let $M = 25\%$, $33\%$, or $50\%$, Table \ref{tbl:separation} shows that in most case of $M$, the majority of projects are above $M$. Hence,  



\begin{RQ}
\textit{\underline{Conclusion:}} With supportive indicators from concurring frequencies and shared artifacts,  
we \textbf{endorse}  that CSc developers intertwine their work
on domain logic and implementation details.
\end{RQ}

\begin{table}[!t]
\centering
\caption{Given $N$ numbers of projects, the percentage of projects having overlapping percentage above a certain threshold (25\%, 33\%, 50\%) are reported for both Scientific and Engineering development.}\label{tbl:separation}
\footnotesize
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c|c|c|c}
\multicolumn{1}{c|}{Overlapping} & \multicolumn{1}{c|}{Development} & \multicolumn{1}{c|}{\% of Projects}& \multicolumn{1}{c|}{\% of Projects}
 & \multicolumn{1}{c}{\% of Projects} \\ %& \multicolumn{1}{c}{} \\
 
 \multicolumn{1}{c|}{Metrics} & \multicolumn{1}{c|}{Type} & \multicolumn{1}{c|}{> 25\%}& \multicolumn{1}{c|}{> 33\%}
 & \multicolumn{1}{c}{> 50\%} \\ %& \multicolumn{1}{c}{Median}\\
\hline
\multirow{2}{*}{\begin{tabular}[c]{c} 
 File
\end{tabular}} & 
 Scientific & 73\% & 64\% & 55\% \\ 
& Engineering & 55\% & 46\% &  27\% \\\hline 
\multirow{2}{*}{\begin{tabular}[c]{c} 
Commit
\end{tabular}} & 
Scientific  & 73\% & 55\% & 37\% \\ 
& Engineering  & 91\% & 73\% & 55\% \\ 
\end{tabular}}
\vspace{-10pt}
\end{table} 

\noindent \textit{\underline{Discussion:}}  It is interesting to note on the reverse relationship of low numbers of overlapped files in engineering enhancemment but low numbers of commits that worked on those shared files in scientific enhancemment. We suspected that engineering enhancement also have particular concerns with extending the usability of the software (e.g. put the software in the cloud, add a front-end interface, environment setup) which generate more distinct files (i.e. low overlapped files percentage). Moreover, those types of activities will most likely interact with core modeling part of the software which requires minimal file-related updates of scientific enhancemment (i.e. low commits percentage that have overlapped files). Table \ref{tbl:separation} also offers a more critical view to this belief: the domain logic relies heavily on implementation details more than vice versa.

A cultural factor to the observed effects is that for scientific developers, there is no incentive to create domain-general abstractions beyond the goals of a research project, due to the largely fixed-term project-specific nature of scientific software development work \cite{johan18_secs}. 

\section{Nature of the Scientific Challenge Beliefs}



% \subsection{Requirements}\label{rments}


% Our analysis of this first belief will conclude that
% CS code is built in an exploratory manner,
% rather than in response to some pre-defined
% requirements. While this first conclusion is hardly
% surprising, it does offer a simple example
% of how this paper uses Github
% data to reason about CS projects.



% \noindent \textit{\underline{Belief:}} According to Basili (and others), in computational science,
% project requirements are not known up front \cite{segal08_ss, carver07_environment, segal05_ss, basili08_hpc, easterbrook_cs}.
% If true, then this belief means that SE methods based on 
% static requirements (e.g. model checking) are not so valuable
% for CS software.





% \noindent \textit{\underline{Notes:}} 
% Many authors, including Carver~\cite{carver07_environment}
% and Easterbrook~\cite{easterbrook_cs}
% comment that CS code is not written in order
% to satisfy some pre-existing set of requirements.
% Rather, it is written an exploratory fashion in order
% to better understand some effects. 
% This would make CS software very different to code developed using (e.g.) a waterfall model where the requirements
% are all known at the start of the development.


% \noindent \textit{\underline{Modeling Assumptions:}} 
% Projects with pre-existing list of fixed requirements 
% can be developed in a ``waterfall'' style.
% When that style is applied,
% requirements is followed by analysis,
% design, code, implementation and test.
% The observable feature of such projects
% is that most of the testing and bug fixing
% activity occurs {\em after} a code base
% has been enhanced with the required
% scientific or engineering functionality.



% \noindent \textit{\underline{Prediction: }} If CS software was written in response to some pre-existing set of requirements, then
% we would expect to see bug-fixing and testing to be a predominately end-stage activity.






% \noindent \textit{\underline{Observed:}} 
% As shown in \fig{belief1}, the rate
% of commits of different types
% is nearly constant across the project
% lifetime. This observation is {\em not} consistent with 
% waterfall-style projects where most of the enhancement work happens early in the lifecycle and most of the test work happens later on.
% \vspace{-2mm}
% \begin{RQ}
% \textit{\underline{Conclusion:}}
% We \textbf{endorse} the belief that, in CS, project requirements are usually not pre-defined
% at the start of a project.
% \end{RQ}
 


\subsection{Verification and Validation is Different}\label{vv}
\noindent \textit{\underline{Belief:}} 
According to Carver et al., and others, 
verification and validation in software development for CSc is difficult and strictly scientific \cite{carver07_environment, kanewala13_testing, carver06_hpc, Prabhu11_cssurvey, basili08_hpc}.
That is, CSc developers spend more time debugging their theories
of physical phenomena than debugging systems issues within their code.
If this belief were true then much of the standard SE testing
infrastructure would need extending before it can be applied to CSc. For example,
while unit tests and system tests are certainly useful, CSc projects would need a separate level of tests for ``physical concept testing''. 

\noindent \textit{\underline{Notes:}} 
According to Carver et al.~\cite{carver07_environment},
verification and validation of scientific software can be difficult for several reasons:
\bi
  \item Lack of suitable test oracles \cite{kanewala13_testing},
  \item Complex distributed hardware environments with no comparable software \cite{basili08_hpc},
  \item Scientists often suspect that the problems of the software is the results of their scientific theory~\cite{faulk09_secs},
  \item Lack of physical experimentation and experimental validation is impractical \cite{carver07_environment}. 
\ei




\noindent\textit{\underline{Modeling Assumptions:}} 
As stated above in \S\ref{model},
in order to bridge between the terminology of the belief and the Github data, we assume that (1) V\&V is associated with testing; (2) the amount of testing is an indicator for V\&V efforts; and (3) the amount of failed tests indicating how rigorous the software is being validated and verified. This second assumption is shared by
other
studies  by Vasilescu et al. \cite{vasilescu16_limit} or Menzies et al. \cite{xia2019sequential}, where the number and the proportion of commits are treated as an indicator for developing efforts of the repositories. For the third assumption, from Travis CI, we can say the amount of failed tests (FT) and failed builds (FB) as indicators for measuring the rigor of V\&V (3) results from the efforts (2).  

\bi
 \item  \textit{FB\_Ratio} = sum(FB) / all\_builds : how often a build fails? 
 \item  \textit{Average\_FT\_per\_FB} = sum(FT) / FB : for each failed build, what is the number of expected failed tests? 
 \item  \textit{FT\_Ratio\_across\_FB} = sum(FT) / sum(Tests\_across\_FB) : what is the proportion of failed tests across all failed build? 
 \item  \textit{FT\_Ratio\_per\_FB} = median(FT\_per\_FB / Tests\_per\_FB) : for each failed build, what is the proportion of failed tests?
\ei

\noindent \textit{\underline{Prediction: }}
Verification and validation in CSc is more
``difficult'' than in SE if the observed CSc efforts (2) in respect to CSc rigor (3) in this area
is larger than in SE. As to ``strictly scientific'', we should see far more ``scientific
testing'' that otherwise (e.g. ``engineering testing''). 


\begin{wraptable}{r}{1.7in}
\vspace{-15pt}
\caption{Labels of testing type commits from the labeled Testing commits.}\label{tbl:testing}
\footnotesize \begin{tabular}{l|c|c}
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{Absolute} & \multicolumn{1}{c}{Percent}\\
\hline
Science & 289 & 47\% \\
Engineering & 146 & 24\% \\
Other & 173 & 29\% 
\end{tabular}
%}
%\end{threeparttable} 
\end{wraptable}\noindent \textit{\underline{Observed:}}
It is easy to show that CSc software verification and validation are heavily focused on scientific issues. Among the commits that were labeled testing, we classified them based on science, engineering, or other testing. The labelling guidelines are similar to the original one for the whole development cycle mentioned in \S\ref{tion:labelling}. We achieved a ``substantial agreement'' with the inter-rater reliability between the two reviewers where the Cohen's $\kappa = 0.78$ \cite{irr_kappa}.
Table \ref{tbl:testing} shows that ``scientific testing'' is the largest type of commit in our labeled Testing commits sample (at 47\%). 
Far less effort is spent on ``engineering testing'' (only 24\%). 


\begin{wraptable}{r}{2in}
\scriptsize
\centering
\vspace{-10pt}
\caption{Median and interquartile range (IQR) summary for three attributes portraying the testing rigor of CSc and SE projects. IQR is the delta between the 75th and 25th percentile.}
\vspace{-5pt}
\label{tbl:testing_travis}
%\resizebox{1\linewidth}{!}{
\begin{tabular}{c|c|c|c}
\textbf{Metric} & \begin{tabular}[c]{@{}c@{}} \textbf{Project} \end{tabular} & \textbf{Median} & \textbf{IQR} \\ \hline
\multirow{2}{*}{\begin{tabular}[l]{c} %\rotatebox[origin=c]{90}{\parbox[c]{0.7cm}{\centering IF Ratio}} 
FB  \end{tabular}} & 
SE & 38\% & 33\% \\ [2pt]
Ratio & CSc & 25\% & 20\% \\ [2pt]
\hline
\multirow{2}{*}{\begin{tabular}[l]{c} %\rotatebox[origin=c]{90}{\parbox[c]{0.7cm}{\centering IF Ratio}} 
FT  \end{tabular}} & 
CSc & 20 & 27 \\ [2pt]
per FB & SE & 11 & 19 \\ [2pt]
\hline
\multirow{2}{*}{\begin{tabular}[l]{c} %\rotatebox[origin=c]{90}{\parbox[c]{0.8cm}{\centering ILOC Ratio}}
FT Ratio 
\end{tabular}} & 
CSc & 3\% & 2\% \\ [2pt]
across FB & SE & 1\% & 7\% \\ [3pt]
\hline
\multirow{2}{*}{\begin{tabular}[l]{c} %\rotatebox[origin=c]{90}{\parbox[c]{0.8cm}{\centering FF Ratio}}
FT Ratio \end{tabular}} & 
CSc & 26\% & 19\% \\ [2pt]
per FB & SE & 20\% & 27\% \\ [2pt]

\end{tabular}%}
\vspace{-10pt}
\end{wraptable}As to showing the CSc verification and validation is ``as least or more difficult'' than in SE,
recall that
  Figure \ref{fig:SE_activities} showed that
  15\%, 6\% percent of the commits
  are associated with CSc, SE testing (respectively).   This SE data comes from a recent study \cite{tu2019better} of the random 20 highly starred from Github that satisfies our sanity checks of Table~\ref{tbl:sanity}.
Note that  15\% is 2.5 times larger than 6\%. Table \ref{tbl:testing_travis} summarizes the median and interquartile range of testing rigor for both CSc and SE projects. Across all four metrics, CSc is reported to test more rigorously than SE. However, by the statistical Scott-Knott test\footnote{
Scott-Knott recursively divides treatments, stopping if
a significance test or an effect size test reports that sub-divisions are
not statistically different~\cite{mittas2013ranking, ghotra15}.
We use a bootstrap test to check for significance differences (at the 95\% confidence level) 
and the $\mathit{A12}$ test to check for small effects ($\mathit{A12}\ge0.6$).
This procedure has been endorsed in the recent SE literature~\cite{mittas2013ranking,arcuri2011practical}.}, the difference in testing rigor is not distinguishable between CSc and SE. This is interesting as we are aware that CSc developers have to put far more efforts in testing than SE developers, yet CSc's testing is as rigor as SE's testing. That is
to say, the nature of verification and validation are more or at least difficult for scientific developers than traditional software engineers.

\begin{RQ}
\textit{\underline{Conclusion:}}
We \textbf{endorse} the belief that within CSc, software development's verification and validation, are difficult and mostly concerned with scientific issues. 
\end{RQ} 

% \bi
% \item CS software is written to correspond to physical phenomena, the nature of which may never change (e.g. the atomic weight of iron).
% \item
% the highly starred projects in Github) is written to correspond to an ever-changing ecology of platforms, tools, user expectations, and newly-arrive AI algorithms, etc.   
% Hence, it is not surprising  SE software requires more verification and validation effort than CS software since the problem it addresses are more dynamic.

\noindent\textit{\underline{Discussion:}} This result is somewhat strange since it runs counter to standard beliefs in the SE literature (e.g. Brookes argues that unit tests and systems tests will consume half the time of any project~\cite{brooks1995mythical}). One of our conjectures include the larger V\&V effort in SE  is due to the nature of CSc problems. CSc software is written to correspond to solve endless nature's problems (most are beyond human's understanding) with the requirements are not known up front and software's state are incrementally improved. CSc V\&V have to cover both scientific and engineering concerns while SE V\&V at some points would mature to only focusing on verification (especially when SE software is based on production focus). 

More intuitively, by looking at the \textit{Testing} and \textit{Bug-fixing} attributes from Figure \ref{fig:SE_activities}, the bug-fixing activities from SE software development are almost three times as in CSc which is the direct result from testing 2.5 times less than CSc. Essentially, the \textit{less} developers test, the \textit{more} bugs developers have to fix. After shipping the software, SE developers are more reluctant to test the software while for CSc developers, scientific software research and development might be a continuous journey.

Moreover, a conflating factor that might make us doubt this observation would be if the CSc codes were always much smaller than the SE codes. If that were true then even if some tasks had a larger percentage effort 
(e.g. Table \ref{tbl:testing}'s ``scientific testing'') then  ``relatively more'' might actually
mean ``less'' (in absolute terms). 
As discussed in \S\ref{tion:data}, our data does not show that  SE projects are larger and more active
than CSc projects.


% Among all the defects fixing, the scientific and engineering defects are at the same rate. Yet, the testing focuses solely on the scientific aspect, almost three times (45\%/17\%), more than engineering testing. In a sense, scientists solely believe that the software is defected due to their science understanding when transferring that to source code while overlooking the engineering aspect. Yet, it is understandable because scientists have a lot of responsibilities (read and write papers, grants, give presentations, develop scientific models, etc) so they can only focus on testing on what they good at, i.e. scientific models. It is possibly useful for the community to incorporate automated SE testing tools for CS projects. 






%Whatever the reason, 
%note that this result calls for a different kind of testing device in CS.  In standard SE, a ``test'' can be something as simple as a unit test (checking if, for example, that subtrees remain in sorted order after insertion).
% But in CS, ``tests'' need to be a higher level and refer back to some core physical properties as defined
% by scientific theory.  


% \noindent \textit{\underline{Threats of Validity:}} CS developers are good at contributing relevant code to the system that less likely to introduce the bugs which make them more confident to spend less time on maintenance. 
 

 

%  \subsection{Formality}~\\
%  \noindent \textit{\underline{Claim:}} Overly Formal Software Processes Restrict Research \cite{easterbrook_cs, segal07_problem, carver07_environment, segal08_ss}.

%  \noindent \textit{\underline{Rationale:}} 
% Scientific software development is deeply embedded into the scientific methods and research fashion where developers will find traditional software development processes with big design upfront (e.g. waterfall approach) very challenging \cite{easterbrook_cs}. 
% As scientific software is evolving continuously no clear-cut requirements analysis, design, or maintenance phases can be discerned \cite{segal07_problem}. Therefore, instead of established SE processes, scientists apply an informal, nonstandard process. 

 
% The scientists regard their informal software process as necessarily following
% from applying the scientific method to scientific reasoning with the help of computing. The process itself has a lot of resemblances with the agile methodology. The process involved:
%  \begin{enumerate}
%      \item starts from a scientific problem and the necessary software or application could be required to solve
%      \item a prototype is developed and continuously improved, guided
% by the questions ``Does it do what I want?'' and ``Does it help solve the scientific problem at
% hand?''
%     \item cursory testing
%     \item modifications till plausible outputs are achieved.

%  \end{enumerate}

%  \noindent \textit{\underline{Observed:}}





% In this section, we discuss characteristics of software development in
% computational science that are due to limitations regarding available computing
% resources and their efficient programming. 





\begin{figure*}[!t]
\vspace{-5pt}
\resizebox{1\linewidth}{!}{
 \hspace{-2mm}
 \includegraphics[width=0.5\linewidth]{img/SE_heroes_commits.png}
 \hspace{2mm}
 \includegraphics[width=0.5\linewidth]{img/CS_heroes_commits.png}}
\vspace{-13pt}
\caption{Percentage of code commits that introduce new bugs, made by hero and non-hero developers from SE (left) and CSc (right) projects.
Each x-axis of that figure is one project. The hero and non-hero defect introduction rate (defects per commit) is the ratio of the blue to red values
at any specific x-value.
}\label{fig:heroes}
\vspace{-7pt}
\end{figure*}

\section{Limitations Due to Cultural Differences Beliefs}

\subsection{Terminology}\label{terms}
\noindent \textit{\underline{Belief:}} 
Vanter et al.~\cite{faulk09_secs, easterbrook_cs, boyle09_lessons} express concerns
that it is hard to translate concepts between  CSc and SE (since the fields are so different). 

\noindent \textit{\underline{Notes:}} When two fields evolve along different
lines (like SE and CSc) it is possible that the terminology of one field has important
differences in the other field. This is worrying if those terminology differences mean that methods from one field perform poorly in the other.



\newcommand{\varendash}[1][5pt]{%
  \makebox[#1]{\leaders\hbox{--}\hfill\kern0pt}%
}

\newcommand{\RULEE}[1]{\textcolor{black!20}{\rule{#1}{6pt}}}
\begin{table}[!t]
\caption{Given $N$ releases of software, this chart shows the percent of releases
where off-the-shelf SE defect predictor is defeated by the
EMBLEM defect predictor (that learns what ``defect'' means for CSc). }
\label{tbl:rq2aaa}
\footnotesize
\begin{tabular}{r|r@{~}l}
Project & \% & wins for EMBLEM\\[0.1cm]

AMBER & 33 &   \RULEE{67pt} \\ 

HOOMD & 60 &  \RULEE{120pt} \\ 

RMG-PY  & 60 &  \RULEE{120pt}  \\ 

\cellcolor{gray!30}   SCIRUN  & 63 &   \RULEE{125pt}  \\ 

ABINIT & 63 &   \RULEE{125pt}  \\ 

\cellcolor{gray!30}  OMPI &  66 &   \RULEE{130pt}  \\ 

LIBMESH & 72 &  \RULEE{140pt}    \\  

MDANALYSIS & 72 &  \RULEE{140pt}   \\ 

LAMMPS & 75 &  \RULEE{150pt}  \\

\cellcolor{gray!30}   PSI4   & 80 &   \RULEE{160pt}  \\ 


XENON & 83 &\RULEE{170pt} 




\end{tabular}
\vspace{-10pt}
\end{table}



\noindent \textit{\underline{Prediction:}} If the belief is held, then the off-the-shelf SE methods may perform badly of CSc projects {\em unless} they first adjust the meaning of their SE terminology.

\noindent \textit{\underline{Observed:}} Tu et al. \cite{tu2019better} found that the concept of ``defect'' was  different in CSc and SE.
Specifically, off-the-shelf defect labeling technologies (that are widely used  in SE \cite{tu2019better,mockus00changeskeys,kamei12_jit, hindle08_largecommits, Kim08changes}),
performed poorly when applied to CSc projects.
To fix that, they built an automatic assistant called EMBLEM that showed a subject matter expert 
examples of supposedly defective CSc code (as identified by the off-the-shelf SE tool).
Using feedback from the subject matter expert, the automatic assistant adjusted the support vectors of an SVM. In this way, the assistant could learner what ``defect'' means in CSc.

Table~\ref{tbl:rq2aaa} 
compares defect predictions generated by a data
miner using defect labels from (a)~an off-the-shelf SE defect method;
and (b)~those generated via EMBLEM. The gray high-lighted ones are additionally added as a precaution step to check the validity of their work. 
Given $N$ releases per project and using data mining, two predictors were learned from release $i$ then tested on release $i+1$:
\bi
\item
One predictor was built on defects identified  via EMBLEM;
\item
The other predictor was built on defects identified by the standard SE defect labeler.
\ei


Note that, in most cases in  Table~\ref{tbl:rq2aaa},  EMBLEM's predictors usually out-performed the off-the-shelf SE method. 
That is, for CSc projects,
better results were obtained after adjusting the meaning of a standard term (``defect'')
taken from SE. Hence, 

\begin{RQ}
\textit{\underline{Conclusion:}} We \textbf{endorse} that the CSc community utilizes different terminologies when describing their work. SE tools
may need to be examined and adjusted before being applied to CSc projects.
\end{RQ}

% \noindent \textit{\underline{Discussion:}}  Tu et al. \cite{tu2019better} indicated a few reasons how their work demonstrates that a different model to learn the language of CS during software development is important. In the case of labeling defective commits, they observed that over half of the commits message were (a) very short (almost three times shorter than CS ones on median) and (b) took the form of
% ``\textit{Bug X: [type\_of\_bug] description\_of\_the\_bug-fixing\_commit}''. Moreover, bug-fixing commits from standard SE projects (79\% in median)
% are three to four times more likely than the median values seen in the CS software (23\%). Both of these suggest that SE project are more standardized and mostly in maintenance mode. It is more convenient  and  possible  to  catch  bug-fixing activities through a few keywords (e.g. bug, fix, error, etc). Whereas, CS developed their software in a more dynamic style with a different way to describe and document their work across the development that requires adaptation of the language in order to understand scientific software development. 



% Further, this result also raises attention for the current bad research fashion of reusing and applying established work without critiques. As seen, off-the-shell SE standard methods did not apply well for CS development data. The methods need to be tuned to specialize how scientific researchers develop their software. 

% The characteristics that are listed in this section result from the cultural
% environment in which scientific software development takes place. This
% environment is shaped, for example, by the training of computational scientists
% and the funding schemes of scientific research projects. 



%\noindent \textit{\underline{Threats of Validity:}} 

  
 
\subsection{Code Understanding} ~\\
\noindent \textit{\underline{Belief:}} According to
Segal et al., and others~\cite{segal07_problem, carver06_hpc, Shull05_parallel, sanders08_risk},
CSc projects are so complex that creating
a shared understanding of that code is difficult. 


\noindent \textit{\underline{Notes:}} Research scientists typically do not produce documentation for the software they implement \cite{segal07_enduser, sanders08_risk}.
Further, there is a high personnel turnover rates in scientific software development \cite{carver06_hpc, segal07_problem}. As a result, there is a concern that CSc software is harder to maintain. 

\noindent \textit{\underline{Modeling Assumptions:}} 
When code is hard to maintain,
developers who do work less frequently with the code are more prone to introduce defects
(rationale: the greater the complexity of the code, the greater the effort required to understand it).
Hence, one measure of the complexity of understanding code
is the difference in defect rates between core developers, also known as ``heroes''~\cite{agrawal2018we, goeminne2011evidence, torres2011analysis, robles2009evolution}, and everyone else.
Heroes are that  20\% group of the developers who usually make 80\% (or more) of the code changes.

(Aside:   Majumder et al.~\cite{majumder19_heroes} found that such heroes are very common in open source projects. Their threshold for ``hero-ness'' are the 20\% of developers
who make 80\% of the changes.).

If the defect rate is much higher for non-heroes, that would
indicate that the code is so complex that it can only
be safely changed by those who have studied it in great detail.


\noindent \textit{\underline{Prediction: }} If CSc code is hard to understand than SE code, we would expect that non-hero CSc programmers would
introduce {\em more} defects into the software than non-hero SE programmers. 




\noindent \textit{\underline{Observed:}} Majumder et al. \cite{majumder19_heroes} checked the heroes projects for both heroes and non-heroes contribution of defects within software development. They found that non-heroes introduced 30 to 90\% more defects per commit (25th-75th percentiles) in SE projects. Those
results can be see Figure \ref{fig:heroes} (left-hand-side).

We repeated their study for our 59 CSc projects.  Figure \ref{fig:heroes} (right-hand-side)
shows those results. Each x-axis of that figure is one project so the hero and non-hero defect introduction rate (defects per commit) is the ratio of the blue to red numbers
at any particular x-value.
In those results, we observe that:
\bi
\item In CSc, only 2/59 projects do non-heroes always introduce new defects with each commit (almost 1/3 SE projects).
\item In SE projects, non-heroes's commits are far more likely (30\%-90\% for 25th-75th percentiles) than heroes to introduce new defects.
\item In CSc projects, commits by non-heroes introduce new defects at nearly the same ratio as heroes (actually, 2\%-6\% less than for 25th-75th percentiles).
\ei
 
CSc non-heroes introduce defects at much lower probability than in SE projects. Hence, we say:

\begin{RQ}
\textit{\underline{Conclusion:}} Measured in terms
of a number of defects introduced by each new commit, we \textbf{doubt} that the shared understanding of ``code'' is more difficult within  CSc projects than SE projects.
\end{RQ}



% \begin{table}
% \small
% \begin{center}
% \caption{Percentiles seen in Figure \ref{fig:heroes}.}
% \label{tbl:heroes}
% \resizebox{1\linewidth}{!}{
% \begin{tabular}{c|c|r@{~}|r@{~}|r@{~}|r@{~}|r@{~}|r@{~}}
% & & \multicolumn{6}{c}{\textbf{Category}} \\
% \cline{3-8}
% &  & \multicolumn{3}{c|}{\textbf{SE Projects}} & \multicolumn{3}{c}{\textbf{CS Projects}}\\
% \cline{3-8}
% \textbf{Metric} & \begin{tabular}[c]{@{}c@{}} \textbf{Percentile} \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{Hero}\end{tabular} & \textbf{Non-Hero} & \begin{tabular}[c]{@{}c@{}} \textbf{Ratio}\end{tabular} & \textbf{Hero} & \textbf{Non-Hero} & \textbf{Ratio} \\ \hline

% \multirow{3}{*}{\begin{tabular}[l]{c} \rotatebox[origin=c]{90}{\parbox[c]{1.5cm}{\centering code interaction}} \end{tabular}}  & 
% 25th & 52 & 67 & 1.3 & 46 & 50 & 1.09  \\ [3pt]
% & 50th & 58 & 75 & 1.3 & 52 & 52 & 1 \\ [3pt]
% & 75th & 53 & 100 & 1.9 & 60 & 60 & 1 \\ [4pt]
% \hline
% \multirow{3}{*}{\begin{tabular}[l]{c} \rotatebox[origin=c]{90}{\parbox[c]{1.5cm}{\centering social interaction}} \end{tabular}}  & 
% 25th & 52 & 67 & 1.3 & 39 & 36 & 0.92  \\[3pt] 
% & 50th & 58 & 75 & 1.3 & 49 & 48 & 0.98 \\ [3pt]
% & 75th & 53 & 100 & 1.9 & 61 & 57 & 0.93 \\ [3pt]
% \end{tabular}}
% \end{center}
% \end{table}




\noindent \textit{\underline{Discussion:}} 
Cai et al. \cite{cai19_debt} argues that number of introduced bugs
per commit is {\em not} a measure of code comprehension.
In their case study, defect rates shot up after refactoring
precisely because (a)~developers now understood the code better so (b)~they were willing to make more changes so (c)~they
introduced more bugs. While their argument is certainly interesting, the Figure \ref{fig:heroes} (right-hand-side) results
are not a statement of defects {\em increased} after changes.
Rather, those results on defects ratios that are {\em the same} between two populations of programmers. 




\subsection{Code Reuse} ~\\
\noindent \textit{\underline{Belief:}} 
Carver et al.~\cite{segal07_problem, carver06_hpc, Shull05_parallel, sanders08_risk} warn that there is little
code reuse in CSc projects

\noindent \textit{\underline{Notes:}} 
Carver et al. report that scientific developers have a history of not adopting or re-use the software developed by others (or even their own). They 
say this is due to:

\bi
\item The structural assumptions from the others would be too strict and narrow \cite{carver06_hpc, basili08_hpc}
\item Most of the software is not built with comprehensibility requirement as the top priorities \cite{segal07_problem}. Hence, adapting old code for new domains is difficult.
\item
CSc scientists believe that their time and efforts can be more conserved by being spent on implementing the new libraries and framework rather than understanding existing frameworks.
\ei
\noindent \textit{\underline{Modeling Assumptions:}} To
measure reuse, we can measure the  code called
via libraries/ packages that come from outside of a repository. This is to say that the amount of external imports (EI) and files that have external imports (FEI) are indications of the reuse activities within the software. There are four attributes for this that we define below. For all of them, the higher the value the better reuse within their projects: 

\bi
\item \textit{IF\_Ratio} = EI / Total\_\#\_of\_Files
\item \textit{ILOC\_Ratio} = EI / LOC (total number lines of code)
\item \textit{FF\_Ratio} = FEI / Total\_\#\_of\_Files
\item \textit{II\_Ratio} = EI / Total\_\#\_of\_Imports 
\ei


\noindent\textit{\underline{Prediction:}} CSc projects have
less reuse than SE projects if the above ratios
are lower to CSc than SE. 


\begin{wraptable}{r}{2.2in}
\vspace{-10pt}
\scriptsize
\centering
\caption{Median and IQR summary for four attributes portraying the reuse state of CSc and SE projects. IQR is the delta between the 75th and 25th percentile.}
\label{tbl:reuse}
%\resizebox{1\linewidth}{!}{
\begin{tabular}{c|c|c|c}
\textbf{Metric} & \begin{tabular}[c]{@{}c@{}} \textbf{Project} \end{tabular} & \textbf{Median} & \textbf{IQR} \\ \hline
\multirow{2}{*}{\begin{tabular}[l]{c} %\rotatebox[origin=c]{90}{\parbox[c]{0.7cm}{\centering IF Ratio}} 
IF Ratio\end{tabular}} & 
CSc & 3.2 & 1.6 \\ [2pt]
& SE & 2.9 & 1.6 \\ [2pt]
\hline
\multirow{2}{*}{\begin{tabular}[l]{c} %\rotatebox[origin=c]{90}{\parbox[c]{0.8cm}{\centering ILOC Ratio}}
ILOC Ratio
\end{tabular}} & 
SE & 13\textperthousand & 9\textperthousand \\ [2pt]
& CSc & 10\textperthousand & 8\textperthousand \\ [3pt]
\hline
\multirow{2}{*}{\begin{tabular}[l]{c} %\rotatebox[origin=c]{90}{\parbox[c]{0.8cm}{\centering FF Ratio}}
FF Ratio\end{tabular}} & 
CSc & 86\% & 19\% \\ [2pt]
& SE & 81\% & 19\% \\ [2pt]
\hline
\multirow{2}{*}{\begin{tabular}[l]{c} %\rotatebox[origin=c]{90}{\parbox[c]{0.7cm}{\centering II Ratio}} 
II Ratio\end{tabular}} & 
SE & 70\% & 27\%  \rule{0pt}{2.5ex} \\ [1.5pt]
& CSc & 55\% & 19\% \\ [1.5pt]
\end{tabular}%}

\end{wraptable} \noindent \textit{\underline{Observed:}} Table \ref{tbl:reuse} summarizes the median and interquartile range for reuse metrics in both CSc and SE projects. The lines of code reuse is low for CSc projects (just 10\%) but its nearly the same as SE projects (13\%). In fact,
after applying a Scott-Knott test, 
we can report that the SE projects are statistically
indistinguishable from CSc projects, on all the metrics of Table \ref{tbl:reuse}. 
That, in this sample, we found no difference in the reuse rates
of SE and CSc code. Hence: 
\begin{RQ} 
\textit{\underline{Conclusion:}} We \textbf{doubt} that CSc reuses less code  than SE. 
\end{RQ}

\noindent \textit{\underline{Discussion:}} 
The ratios used here only reflect on code reuse.
Other kinds of reuse include design or conceptual reuse. 
Also missed by the above ratios is non-verbatim reuse (where code is reused, but modified).
Further, the above ratios may miss certain important code measures
(e.g. text-based, token-based, tree-based, metric-based, semantic and hybrid).

We did not explore those additional measures of reuse since
their implementation leads to $O(n^{(m-1)})$ complexity with $n$ as the current section of codes within the project and $m$ is the number of the projects to compare to. We hence leave reuse measurement in CSc to  future work.

\subsection{Low Perceived Value} ~\\
\noindent \textit{\underline{Belief:}} 
Easterbrook et al. comment that even though CSc codes
may be maintained for many years,
they are not perceived to have value within their own community~\cite{faulk09_secs, segal07_enduser, easterbrook_cs, boyle09_lessons}.

\noindent \textit{\underline{Notes:}} The social structures of the computational science community
typically reward new conclusions about physical phenomena much more than
details about the software used
to make those conclusions. This raises the concern since, as said in the introduction, the software is just as important a tool for modern science as, say, the test tube. However, scientific developers perceive no value in that software, then the software will be built and maintained in a sub-optimum manner~\cite{sanders08_risk}. 

\noindent \textit{\underline{Modeling Assumptions:}} 
One measure of software perceived value is its associated popularity within Github. This can be measured in many ways such as ratio of open to closed issues, or
numbers of stars or watchers or tags or forks. By consider the arrival rate of these measures with respect to the duration variable, and comparing those numbers between CSc and SE, we can comment on how actively popular is a CSc project compared to SE.

\noindent \textit{\underline{Prediction:}} 
According to this belief, CSc projects should not be so actively popular as SE projects.



\noindent \textit{\underline{Observed:}} 
The Figure \ref{fig:comparison} showed a
level of activity for CSc projects that rivals that of SE.
Except for duration, most of the indicators are similar or larger for CSc than SE
(recall that even when the median CSc results were lower, statistical tests showed that those differences
were not significantly distinguishable). Note that several of these indicators could be seen
to measure the popularity of a project. For example,
there are more closed releases that open issues which mean someone cares enough to work those issues.
Overall we can say:


\begin{RQ}
\textit{\underline{Conclusion:}} 
We  \textbf{doubt} that CSc software is perceived by its community as having less value,
as compared to standard SE software.
\end{RQ}

\noindent \textit{\underline{Discussion:}} 
%Scientific software is developed by mostly Ph.D. students and postdocs that have high personnel turnover rates \cite{johan18_secs}. Moreover, due to grant-based funding schemes and research natures, the long-term vision is discouraged with ``quick and dirty'' solutions are more likely to be favored \cite{boyle09_lessons} which leads to short-lived software. 
% The result for the value aspect may surprise some folks. However, a trend in software-based research for both scientific developers or professional end-user is following the development of the projects and ultimately use such a project to curate necessary data, baseline results, and integrated framework that are relevant to their studies. Hence, the signal is clear by observing the similar distribution of \textit{Stars}, \textit{Forks}, and \textit{Watchers} (i.e. for following) and the greater distribution of \textit{Open \& Closed Issues} (i.e. for using) from CSc's metrics to SE's metrics. 
% \noindent \textit{\underline{Threats of Validity:}} 
One threat to the validity of the above conclusion is that all of our sample of CSc projects come from Github projects.
 There exist older existing systems and commercial projects that are not housed on Github. It is possible that those other systems are less popular than standard SE software. This would be an interesting area for future research.
  
  

%V
%comparison

% \noindent \textit{\underline{Threats of Validity:}} 






\subsection{Limited SE Training} ~\\
\noindent \textit{\underline{Belief:}} According to Segal et al.,
and others\cite{segal07_enduser, basili08_hpc, carver13_perception, easterbrook_cs, sanders08_risk}, few CSc scientists are trained in SE.
This is a concern since that lack of training might lead to sub-optimum software
development practices.

\noindent \textit{\underline{Notes:}} 
The people who write the CSc code usually
receive their degrees in
astronomy, astrophysics, chemistry, economics, genomics, molecular biology, oceanography, physics, political science, and many engineering fields.
That is, the primary field of study for these developers is {\em not}
software engineering. For many of these people,
learning SE is perceived as an excessive additional burden\cite{boyle09_lessons}. 

\noindent \textit{\underline{Modeling Assumptions:}} 
Successful training in SE is shown in
\bi
\item An {\em efficient} software process is one that allows  people to work together, faster. 
\item The general quality of the software; e.g. the number of projects that pass the sanity checks of Table~\ref{tbl:sanity}.
\item The adoption of SE practices (e.g. an incremental development styles) can be inferred by comparing the distributions of different software development efforts between SE \& CSc.
\ei

\noindent \textit{\underline{Prediction:}} If this belief is valid, then more CSc
projects should be poorly managed. Consequently, they would be less efficient. Also, fewer of them should
pass the sanity checks of Table~\ref{tbl:sanity}.
Further, we would not be able to detect current SE  practices within the CSc project Github data.

\noindent \textit{\underline{Observed:}}
Recalling  the discussion about 
Figure~\ref{fig:comparison},
the case was made about if \S\ref{tion:data} that the CSc development community
seems more {\em efficient} (as defiend above) than SE. 

As to the sanity checks, 
two samples were used:
\bi
\item
We applied the sanity checks
of Table~\ref{tbl:sanity} to the 678 CSc projects from~\S\ref{tion:data}. This selected 59 CSc projects.
\item
Also, we took 50,000 SE Github projects (selected at random) and applied
the same sanity checks. This selected 1,300 projects.
\ei
This means that CSc projects are over three times more likely to be {\em sane}: 

\centerline{\scalebox{1.1}{$\frac{\mathit{CSc\_post\_pre\_sanity\_rate}}{\mathit{SE\_post\_pre\_sanity\_rate}} = \frac{\frac{\mathit{CSc\_post\_sanity}}{\mathit{CSc\_pre\_sanity}}}{\frac{\mathit{SE\_post\_sanity}}{\mathit{SE\_pre\_sanity}}} = $} \scalebox{1.5}{$\frac{\frac{59}{678}}{\frac{1,300}{50,000}} = $} 3.35 }

\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth, height=1.5in]{img/sanity.png} 
  \caption{SE and CSc projects count before \& after sanity checks.}
  \label{fig:sanity}
  \vspace{-10pt}
\end{figure}
 Also, recalling Figure \ref{fig:belief1} CSc developers are observed to have a near-constant growth rate in their number of enhancements across their entire lifecycle. This observation is consistent with CSc developers using
  contemporary continuous agile practices. 

From these observations, the picture that emerges is that CSc developers are very good at adopting contemporary  SE approaches to SE development. More specifically, CSc developers
use software engineering best practices   at least as much (and perhaps even more) that SE developers. Hence, we say: 
\begin{RQ}
\textit{\underline{Conclusion:}} We \textbf{doubt} this a lack of formal training in SE is inhibiting CSc development
\end{RQ}

% \noindent \textit{\underline{Discussion:}} The view of training in SE for us is not solely about data structures and algorithm understandings (i.e. may not be even applicable when entering SE career in Sillicon Valley). It is about the real-world experience and intuitions that scientists can also pick up from working with colleagues (e.g. 1.a belief: requirements are not known upfront, agile philosophy). Similar to \S4.1's \textit{Discussion}, CSc developers are well aware that SE skills are appealing to high-profile companies in the industry.  

% Moreover, we also offer additional evidence to doubt this belief in \S5.2 and \S5.3. CSc novices and outsiders understand the source code than SE ones to contribute significantly less defects. CSc projects reuse as much code as SE projects.   

%\subsection{External Validity}

%Methodology to understand the difference between software development domains in SE. Not our final conclusions or definitions, all are threatened by external validities but our conclusions are reproducible and our analysis can be repeated when new data arrives. 

\section{Discussion}
This section reflects on the 13 beliefs study means for
applying SE methods to CSc.

Firstly, there is much SE that can be applied to CSc. We saw many times in this study that CSc developers are very interested and aware of SE methods (e.g. agile philosophy in \S4.1 and modern techniques in \S5.1). Computational Science is a rich domain within which SE tools can be very useful.

That said,  we offer one word of caution about moving SE's tools and methods to CSc. The discussion in \S\ref{terms} warned that basic terminology can be different in SE to CSc. It is, therefore, wise to check domain terminology. The incremental data mining tool described in \S\ref{terms} is one way to reduce the time and cost
involved in performing such checks. More generally, this calls for attention to not apply off-the-shelf method when moving to a different community, it is more useful to tailor SE methods for the CSc community.

Secondly, contrary to much prior pessimism, the overall message of this paper is that CSc software development is at least as successful as standard development practices seen in SE projects (e.g. code understanding in \S6.2, perceived value in \S6.4, and overall SE background in \S6.5). This means that while CSc can take useful tools and insights for SE, there is also room for insights and tools to flow backward from CSc to help SE.
In particular, the relatively lower defect introduction rates seen in Figure~\ref{fig:heroes} are worthy of further study. Perhaps there is something SE can learn from CSc about how to design systems that are less buggy.

%Thirdly, recalling the discussion about lack of requirements in \S\ref{rments}, it would appear that better methods for requirements engineering may not the most cost-effective thing that SE can offer CS. To be sure, in some CS domains such as hydrology (where CS developers work closely with civil engineers), there is space for better requirements engineering. But overall, \S\ref{rments} is saying that if there is only {\em one} thing you try to improve, changes to requirements engineering many not yield the most benefits for CS.

As to other parts of the development lifecycle,  recalling the discussion about verification and validation in \S\ref{vv}, CSc would most benefit from a different kind of testing device. Standard SE is to divide testing into the unit and system testing. \S\ref{vv} says there is a third layer of testing that we might call science testing. CSc debug tools need to be augmented with (e.g.)  physical knowledge that can detect violations of physical properties.

Apart from requirements and testing, another major part of the software lifecycle is development and operations. These are two areas that seem to offer the most benefit for new research. For example, many CSc projects are ``glue'' codes that allow other people to run their experimental application code on some complex platform (with one of the benefit being accessible to other tools within the research community). When that code crashes, it is a triage problem to decide which team needs to fix the code (the ``glue'' developers or the application developers). This is one example of the kind of operational support that would be beneficial to explore. Clearly, there are many more possibilities in this exciting area. In fact, the four beliefs that we did not get to investigate, mentioned in \S2.4, in this study will be our future work in action right away. 




\section{Conclusion}

%he premise of the prior research \cite{johan18_secs} is underlying shortcomings of existing approaches for bridging the gap between Software Engineering and Computational Science can be identified by the 13 recurring characteristics or beliefs.
Through a quantitative investigation on 59 projects, we have found several disconnects between current data
and some-held beliefs about computational science.
Why are so many of those older beliefs not supportable?
We argue that  the  nature of the CSc software development is changing. 
For example, contrary to much prior pessimism,
CSc developers are now very
aware of SE methods. 
We can see much evidence that CSc developers are making
extensive use of SE methods.

 
 The current work here lays out highlighted perspectives, quantitative evidence to clarify existing beliefs about scientific software development.
 We hope these results  prompt a fresh examination of the nature of SE in  CSc which, in turn, might suggests 
  new   specialized supporting tools for CSc.
For example, requirements and unit and system testing  are considered hot topics in in the SE community.
But for CSc projects,   studying  (a)~scientific testing (b)~development and
(c)~operations might be comparatively more useful.



% Therefore, more
% research on this topic is needed, especially to empirically evaluate the 
% gains in productivity and credibility achieved for scientific software by such
% SE approaches. 


\section{Acknowledgments}

We thank the   CSc community
from the Molecular Sciences Software Institute (MOLSSI), and the Science Gateways Community Institute (SGCI)) for
their assistance with this work.


This work was partially funded by 
%an NSF CISE Grant \#1826574 and \#1931425.
blinded for review.

\balance
\bibliographystyle{IEEEtran}
\bibliography{sample.bib}

\end{document}
